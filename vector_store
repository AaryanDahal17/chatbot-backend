import os
from dotenv import load_dotenv
from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain_community.vectorstores import FAISS
from document_split import return_splits

load_dotenv()

def get_or_create_vector_store(cache_dir="vector_store_cache"):
    # Check if cache exists
    if os.path.exists(cache_dir):
        print("Loading vector store from cache...")
        return FAISS.load_local(
            cache_dir, 
            HuggingFaceInferenceAPIEmbeddings(
                api_key=os.getenv("HF_KEY"),
                model_name="sentence-transformers/all-MiniLM-l6-v2"
            ),
            allow_dangerous_deserialization=True  # Enable deserialization since we trust our own cache
        )
    
    # If no cache, create new vector store
    print("Creating new vector store...")
    embeddings = HuggingFaceInferenceAPIEmbeddings(
        api_key=os.getenv("HF_KEY"),
        model_name="sentence-transformers/all-MiniLM-l6-v2"
    )
    
    splits = return_splits()
    vector_store = FAISS.from_documents(splits, embeddings)
    
    # Save to cache
    vector_store.save_local(cache_dir)
    return vector_store

# Initialize vector store
vector_store = get_or_create_vector_store()

def search_document(query, k=5):
    similar_docs = vector_store.similarity_search(query,k=k)
    return similar_docs
    



# similar_docs = search_document("What are the ways to register a business?")
# for doc in similar_docs:
#     print(doc.page_content)
#     print("-"*100)